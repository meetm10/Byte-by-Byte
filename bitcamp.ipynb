{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/meetmukadam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from google.cloud import language\n",
    "from googleapiclient import discovery\n",
    "import httplib2\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import os\n",
    "from tika import parser\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.path.expanduser('creds.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(text):\n",
    "    pos = {}\n",
    "    DISCOVERY_URL = ('https://{api}.googleapis.com/'\n",
    "     '$discovery/rest?version={apiVersion}')\n",
    "    http = httplib2.Http()\n",
    "    credentials = GoogleCredentials.get_application_default().create_scoped(\n",
    "     ['https://www.googleapis.com/auth/cloud-platform'])\n",
    "    http=httplib2.Http()\n",
    "    credentials.authorize(http)\n",
    "    service = discovery.build('language', 'v1beta1',\n",
    "     http=http, discoveryServiceUrl=DISCOVERY_URL)\n",
    "    service_request = service.documents().analyzeSyntax(\n",
    "    body={\n",
    "     'document': {\n",
    "     'type': 'PLAIN_TEXT',\n",
    "     'content': text\n",
    "     }\n",
    "    })\n",
    "    response = service_request.execute()\n",
    "    for token in response['tokens']:\n",
    "#         print(\"{} -> {}\".format(token['text']['content'],token['partOfSpeech']['tag']))\n",
    "        pos[token['text']['content']]  = token['partOfSpeech']['tag']\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL TEXT: ['A', 'linked', 'list', 'is', 'a', 'linear', 'data', 'structure', 'where', 'each', 'element', 'is', 'a', 'separate', 'object', 'Linked', 'list', 'elements', 'are', 'not', 'stored', 'at', 'contiguous', 'location', 'the', 'elements', 'are', 'linked', 'using', 'pointers', 'Each', 'node', 'of', 'a', 'list', 'is', 'made', 'up', 'of', 'two', 'items', '-', 'the', 'data', 'and', 'a', 'reference', 'to', 'the', 'next', 'node', 'The', 'last', 'node', 'has', 'a', 'reference', 'to', 'null', 'The', 'entry', 'point', 'into', 'a', 'linked', 'list', 'is', 'called', 'the', 'head', 'of', 'the', 'list', 'It', 'should', 'be', 'noted', 'that', 'head', 'is', 'not', 'a', 'separate', 'node', 'but', 'the', 'reference', 'to', 'the', 'first', 'node', 'If', 'the', 'list', 'is', 'empty', 'then', 'the', 'head', 'is', 'a', 'null', 'reference']\n",
      "\n",
      "AFTER REMOVING STOP WORDS: linked list linear data structure element separate object Linked list elements stored contiguous location elements linked using pointers node list made two items - data reference next node last node reference null entry point linked list called head list noted head separate node reference first node list empty head null reference \n",
      "\n",
      "UNIGRAMS: {'pointers', 'location', 'point', 'element', 'linear', 'last', 'null', 'entry', 'stored', 'Linked', 'noted', 'linked', 'list', 'using', 'items', 'next', 'first', 'elements', '-', 'called', 'empty', 'made', 'head', 'data', 'node', 'structure', 'separate', 'object', 'reference', 'two', 'contiguous'}\n",
      "\n",
      "BIGRAMS: {'node list', 'empty head', 'separate node', 'last node', 'location elements', 'items -', 'node last', 'null reference', 'object Linked', 'pointers node', 'list called', 'Linked list', 'stored contiguous', 'elements linked', 'using pointers', 'reference first', 'reference next', 'head list', 'null entry', 'linked list', 'structure element', 'point linked', 'list elements', 'elements stored', 'made two', 'node reference', 'called head', 'next node', 'noted head', 'first node', 'separate object', 'data structure', 'head null', 'entry point', 'linked using', 'linear data', 'list noted', 'head separate', 'two items', 'list made', 'data reference', 'reference null', 'element separate', 'list linear', '- data', 'list empty', 'contiguous location'}\n",
      "\n",
      "TRIGRAMS: {'data reference next', 'entry point linked', 'called head list', 'next node last', 'head null reference', 'separate object Linked', 'list called head', 'pointers node list', 'node list made', 'linear data structure', 'items - data', 'reference next node', '- data reference', 'empty head null', 'made two items', 'point linked list', 'head separate node', 'list elements stored', 'using pointers node', 'linked list linear', 'location elements linked', 'structure element separate', 'stored contiguous location', 'list linear data', 'linked using pointers', 'list made two', 'linked list called', 'null entry point', 'node reference first', 'data structure element', 'elements stored contiguous', 'reference null entry', 'reference first node', 'object Linked list', 'separate node reference', 'first node list', 'noted head separate', 'head list noted', 'list empty head', 'two items -', 'node last node', 'elements linked using', 'node reference null', 'last node reference', 'element separate object', 'contiguous location elements', 'node list empty', 'list noted head', 'Linked list elements'}\n"
     ]
    }
   ],
   "source": [
    "# text = \"A linked list is a list which is linked and thus is called linked list A linked list is not a stack A stack is not a linked list A stack can be implemented using linked list but linked list cannot be implemented using stack\"\n",
    "text = 'A linked list is a linear data structure where each element is a separate object. Linked list elements are not stored at contiguous location the elements are linked using pointers. Each node of a list is made up of two items - the data and a reference to the next node. The last node has a reference to null. The entry point into a linked list is called the head of the list. It should be noted that head is not a separate node but the reference to the first node. If the list is empty then the head is a null reference.'\n",
    "# text = 'Hashing is a technique that is used to uniquely identify a specific object from a group of similar objects. Some examples of how hashing is used in our lives include: In universities, each student is assigned a unique roll number that can be used to retrieve information about them. In libraries, each book is assigned a unique number that can be used to determine information about the book, such as its exact position in the library or the users it has been issued to etc. In both these examples the students and books were hashed to a unique number. Assume that you have an object and you want to assign a key to it to make searching easy. To store the key/value pair, you can use a simple array like a data structure where keys (integers) can be used directly as an index to store values. However, in cases where the keys are large and cannot be used directly as an index, you should use hashing. In hashing, large keys are converted into small keys by using hash functions. The values are then stored in a data structure called hash table. The idea of hashing is to distribute entries (key/value pairs) uniformly across an array. Each element is assigned a key (converted key). By using that key you can access the element in O(1) time. Using the key, the algorithm (hash function) computes an index that suggests where an entry can be found or inserted.'\n",
    "\n",
    "text = text.replace('.','')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "upper = set([st.upper() for st in stop_words])\n",
    "for st in upper:\n",
    "    stop_words.add(st)\n",
    "# print(stop_words)\n",
    "\n",
    "text = text.split(' ')\n",
    "print('ORIGINAL TEXT:',text)\n",
    "new_text = ''\n",
    "for t in text:\n",
    "    if t.lower() not in stop_words:\n",
    "        new_text += t+' '\n",
    "print('\\nAFTER REMOVING STOP WORDS:',new_text)\n",
    "# pos_tag_text = pos_tag(word_tokenize(new_text))\n",
    "# print('\\nPOS tag:',pos_tag_text)\n",
    "# print('text after stop word removal:',new_text)\n",
    "\n",
    "#unigrams\n",
    "tokens = nltk.word_tokenize(new_text)\n",
    "unigrams = [x[0] for x in ngrams(tokens,1)]\n",
    "print('\\nUNIGRAMS:',set(unigrams))\n",
    "\n",
    "k = 0\n",
    "most_freq_unigram = []\n",
    "for i in range(k):\n",
    "    ugram = max(list(unigrams),key=list(unigrams).count)\n",
    "    unigrams = list([u for u in list(unigrams) if u!=ugram])\n",
    "    most_freq_unigram.append(ugram[0])\n",
    "# print('most freq unigram:',most_freq_unigram)\n",
    "\n",
    "#bigrams\n",
    "tokens = nltk.word_tokenize(new_text.strip())\n",
    "bigram = nltk.bigrams(tokens)\n",
    "bigrams = []\n",
    "for b in list(bigram):\n",
    "    string = ''\n",
    "    for w in b:\n",
    "        string += w+' '\n",
    "    bigrams.append(string.strip())\n",
    "print('\\nBIGRAMS:',set(bigrams))\n",
    "\n",
    "k = 0\n",
    "most_freq_bigram = []\n",
    "for i in range(k):\n",
    "    bgram = max(list(bigrams),key=bigrams.count)\n",
    "    bigrams = list([b for b in list(bigrams) if b!=bgram])\n",
    "    most_freq_bigram.append(bgram)\n",
    "# print('most freq bigram:',most_freq_bigram)\n",
    "\n",
    "#trigrams\n",
    "trigram = nltk.trigrams(tokens)\n",
    "trigrams = []\n",
    "for t in list(trigram):\n",
    "    string = ''\n",
    "    for w in t:\n",
    "        string += w+' '\n",
    "    trigrams.append(string.strip())\n",
    "print('\\nTRIGRAMS:',set(trigrams))\n",
    "\n",
    "k = 0\n",
    "most_freq_trigram = []\n",
    "for i in range(k):\n",
    "    tgram = max(list(trigrams),key=trigrams.count)\n",
    "    trigrams = list([t for t in list(trigrams) if t!=tgram])\n",
    "    most_freq_trigram.append(tgram)\n",
    "# print('most freq trigram:',most_freq_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_set = set(unigrams)\n",
    "bigram_set = set(bigrams)\n",
    "trigram_set = set(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN UNIGRAMS: ['pointers', 'location', 'point', 'element', 'entry', 'list', 'items', 'elements', 'head', 'data', 'node', 'structure', 'object', 'reference']\n"
     ]
    }
   ],
   "source": [
    "noun_unigrams = []\n",
    "for u in unigram_set:\n",
    "    pos = pos_tagger(u)\n",
    "    if pos[u] == 'NOUN':\n",
    "        noun_unigrams.append(u)\n",
    "print('NOUN UNIGRAMS:',noun_unigrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
